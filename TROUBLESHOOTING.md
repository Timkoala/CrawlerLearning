# 爬虫任务启动故障排除指南

## 常见问题及解决方案

### 1. "启动任务时发生错误" - 前端错误

#### 问题现象
- 点击"开始爬取"按钮后，弹出"启动任务时发生错误"提示
- 浏览器控制台显示网络错误或JavaScript错误

#### 可能原因及解决方案

**1.1 网络连接问题**
- **现象**: 浏览器控制台显示网络错误 (如404, 500等)
- **解决方案**: 
  - 检查Flask应用是否正在运行
  - 确认API端点URL是否正确
  - 检查防火墙设置

**1.2 API认证失败**
- **现象**: 浏览器控制台显示401 Unauthorized错误
- **解决方案**:
  - 检查JavaScript中的API密钥是否正确
  - 确认后端认证模块配置
  - 验证环境变量设置

**1.3 JavaScript错误**
- **现象**: 浏览器控制台显示JavaScript运行时错误
- **解决方案**:
  - 检查JavaScript代码语法
  - 确认DOM元素是否存在
  - 查看具体错误信息定位问题

### 2. "启动任务失败: {具体错误信息}" - 后端错误

#### 问题现象
- 点击"开始爬取"按钮后，弹出具体的错误信息

#### 可能原因及解决方案

**2.1 数据库连接失败**
- **错误信息**: "Database connection failed" 或类似信息
- **解决方案**:
  - 检查数据库文件权限
  - 确认数据库路径是否正确
  - 验证SQLite是否正确安装

**2.2 任务不存在**
- **错误信息**: "404 Not Found" 或 "Job not found"
- **解决方案**:
  - 检查任务ID是否正确
  - 确认任务是否已保存到数据库
  - 验证数据库查询逻辑

**2.3 并发限制超限**
- **错误信息**: "Maximum concurrent jobs reached"
- **解决方案**:
  - 停止一些正在运行的任务
  - 调整最大并发任务数限制
  - 检查进程管理器配置

**2.4 进程启动失败**
- **错误信息**: "Failed to start process" 或 "Permission denied"
- **解决方案**:
  - 检查系统权限设置
  - 确认Python环境配置
  - 验证multiprocessing模块是否正常工作

### 3. Scrapy相关问题

#### 3.1 Scrapy环境配置问题
- **现象**: 爬虫进程无法启动或立即失败
- **解决方案**:
  - 确认Scrapy是否正确安装: `pip install scrapy`
  - 检查Scrapy设置文件配置
  - 验证中间件和管道配置

#### 3.2 爬虫导入错误
- **现象**: 爬虫进程启动时报模块导入错误
- **解决方案**:
  - 检查Python路径设置
  - 确认项目结构和模块导入
  - 验证__init__.py文件是否存在

#### 3.3 爬虫运行时错误
- **现象**: 爬虫进程启动后立即崩溃
- **解决方案**:
  - 查看爬虫日志输出
  - 检查爬虫类定义
  - 验证目标URL是否可访问

## 调试步骤

### 步骤1: 检查前端错误
1. 打开浏览器开发者工具 (F12)
2. 切换到"Console"标签页
3. 点击"开始爬取"按钮
4. 观察控制台输出的错误信息

### 步骤2: 检查网络请求
1. 打开浏览器开发者工具 (F12)
2. 切换到"Network"标签页
3. 点击"开始爬取"按钮
4. 查看发送的API请求及其响应

### 步骤3: 检查后端日志
1. 查看Flask应用的终端输出
2. 观察是否有错误信息或异常堆栈
3. 检查日志级别设置

### 步骤4: 验证数据库状态
1. 使用SQLite工具打开`crawler.db`文件
2. 检查`crawl_job`表中的任务数据
3. 确认任务状态字段值

### 步骤5: 测试Scrapy独立运行
1. 创建简单的测试爬虫脚本
2. 尝试独立运行爬虫
3. 观察是否出现相同错误

## 日志分析

### 前端日志
```
// 成功请求示例
Response status: 200
Response data: {success: true, job: {...}}

// 失败请求示例
Response status: 500
Response data: {success: false, error: "具体错误信息"}
```

### 后端日志
```
// 成功启动任务
[INFO] Starting crawl job 1
[INFO] Job 1 started successfully

// 失败启动任务
[ERROR] Failed to start job 1: 具体错误信息
```

## 配置检查清单

### 环境配置
- [ ] Python 3.7+ 已安装
- [ ] Flask 已安装
- [ ] Scrapy 已安装
- [ ] SQLAlchemy 已安装
- [ ] 依赖包已安装 (`pip install -r requirements.txt`)

### 数据库配置
- [ ] `crawler.db` 文件存在且可写
- [ ] 数据库表已创建
- [ ] 数据库连接字符串正确

### API配置
- [ ] API密钥配置正确
- [ ] 路由端点可访问
- [ ] 认证装饰器正常工作

### 爬虫配置
- [ ] Scrapy设置文件正确
- [ ] 中间件配置无误
- [ ] 管道配置正确
- [ ] 爬虫类可导入

## 常用调试命令

### 检查Python环境
```bash
python --version
pip list | grep -E "(flask|scrapy|sqlalchemy)"
```

### 检查数据库
```bash
sqlite3 crawler.db ".tables"
sqlite3 crawler.db "SELECT * FROM crawl_job LIMIT 5;"
```

### 测试Scrapy
```bash
scrapy version
# 创建简单测试爬虫并运行
```

### 检查进程
```bash
# Windows
tasklist | findstr python

# Linux/Mac
ps aux | grep python
```

## 性能监控

### 系统资源
- CPU使用率
- 内存占用
- 磁盘I/O
- 网络带宽

### 应用性能
- 数据库查询时间
- API响应时间
- 爬虫执行时间
- 并发任务数

## 安全检查

### API安全
- [ ] API密钥保护已启用
- [ ] 请求验证已实现
- [ ] 错误信息不泄露敏感信息

### 数据安全
- [ ] 数据库文件权限设置
- [ ] 敏感信息不记录日志
- [ ] 输入数据已验证和清理

## 升级和维护

### 版本兼容性
- Flask版本兼容性
- Scrapy版本兼容性
- Python版本兼容性

### 定期维护
- 清理过期任务数据
- 监控系统性能
- 更新依赖包版本
- 备份数据库文件

## 联系支持

如果以上步骤都无法解决问题，请提供以下信息：
1. 完整的错误信息
2. 浏览器控制台输出
3. 后端应用日志
4. 系统环境信息
5. 重现步骤